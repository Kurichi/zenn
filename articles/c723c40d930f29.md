---
title: "\"本当に\"ゼロから作るDeep Learning"
emoji: "🔖"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["機械学習", "cpp"]
published: false
---
# はじめに
この記事は筆者がDeep Learningの理解のため、C++を用いてゼロからDeep Learningを実装してみるというものです。
タイトルからもわかる通り、OREILLY出版の「[ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)」(以下：原著）を元に進めていきます。
本記事では原著の第3章から進めていきます。

また、Githubにソースコードもあげているので良かったら参照してください。
[Github](https://github.com/Kurichi/DeepLearning-Zero)

### 環境
OS : Ubuntu20.04 on WSL2
コンパイラ : x86_64-linux-gnu
GCC バージョン : gcc 9.4.0
C++ バージョン : c++20

### レギュレーション
基本的に標準ライブラリのみから実装します。
例外として以下の2つは使います。
- boost
- [matplotlib-cpp](https://github.com/lava/matplotlib-cpp)


### 注意
ソースコードの解説を除く、ネットワーク構造そのものの解説などは基本的に行いません。
また、筆者はC++初心者であり、冗長な書き方や非推奨な書き方をしている可能性があります。
その場合は指摘していただけると助かります。

# matplotlib-cppの設定
グラフを描画するためにmatplotlib-cppがあると便利なので入れます。
以下のコマンドで取ってきてください
```bash
wget https://github.com/lava/matplotlib-cpp/matplotlibcpp.h
```
CMakeを使うのでない場合は入れておいてください。
CMakeLists.txtはGithubを参照してください。

# ニューラルネットワーク
## 活性化関数
原著では活性化関数として、ステップ関数、シグモイド関数、ReLU関数が実装されています。
ここでもそれに倣って3つの関数を実装してみます。
ここで紹介する3つの活性化関数は単純なので簡単に実装できますね。
### ステップ関数
```cpp:ActivationFunction.hpp
namespace deepLearningZero {
template <typename T>
concept Number = std::integral<T> or std::floating_point<T>;

// Step function
template <Number N>
std::vector<N> step_function(const std::vector<N> &v) {
  std::vector<N> result = v;
  for (N &value : result) value = value > 0;
  return result;
}

}  // namespace deepLearningZero
```
```cpp:main.cpp
#include <iostream>

#include "Inc/matplotlibcpp.h"
#include "NeuralNetwork/ActivationFunction.hpp"
namespace dl = deepLearningZero;
namespace plt = matplotlibcpp;

int main() {
  std::vector<double> x(100);
  auto itr = x.begin();
  for (double v = -5; v < 5; v += 0.1) *(itr++) = v;

  auto y = dl::sigmoid(x);
  plt::plot(x, y);
  plt::show();
  return 0;
}
```
![](/images/Step_Function.png)

### シグモイド関数
```cpp:ActivationFunction.hpp
// Sigmoid
template <Number N>
std::vector<double> sigmoid(const std::vector<N> &v) {
  std::vector<double> result = v;
  for (auto &value : result) value = 1 / (1 + exp(-value));
  return result;
}
```
```cpp:main.cpp
int main() {
  std::vector<double> x(100);
  auto itr = x.begin();
  for (double v = -5; v < 5; v += 0.1) *(itr++) = v;

  auto y = dl::sigmoid(x);
  plt::plot(x, y);
  plt::show();
  return 0;
}
```
![](/images/Sigmoid.png)

### ReLU関数
```cpp:ActivationFunction.hpp
template <Number N>
std::vector<N> ReLU(const std::vector<N> &v) {
  std::vector<N> result = v;
  for (auto &value : result) value = std::max((N)0, value);
  return result;
}
```
```cpp:main.cpp
int main() {
  std::vector<double> x(100);
  auto itr = x.begin();
  for (double v = -5; v < 5; v += 0.1) *(itr++) = v;

  auto y = dl::ReLU(x);
  plt::plot(x, y);
  plt::show();
  return 0;
}
```
![](/images/ReLU.png)

## ベクトル積
ベクトル積は必ずと言っていいほど使うのでここで実装しておきます。
```cpp:NumCpp.hpp
namespace numcpp {
template <typename T>
concept Number = std::integral<T> or std::floating_point<T>;

template <Number T>
std::vector<std::vector<T>> dot(const std::vector<std::vector<T>> &a,
                                const std::vector<std::vector<T>> &b) {
  const int high = a.size();
  const int common = b.size();
  const int width = b[0].size();
  std::vector<std::vector<T>> result(high, std::vector<T>(width, 0));
#pragma omp parallel for
  for (int i = 0; i < high; i++) {
    for (int j = 0; j < width; j++) {
      for (int k = 0; k < common; k++) {
        result[i][j] += a[i][k] * b[k][j];
      }
    }
  }

  return result;
}

}  // namespace numcpp
```
```cpp:main.cpp
int main() {
  std::vector<std::vector<int>> a({{0, 1}, {2, 3}, {4, 5}});
  std::vector<std::vector<int>> b({{0, 1, 2, 3}, {4, 5, 6, 7}});
  auto r = nc::dot(a, b);
  for (int i = 0; i < r.size(); i++) {
    for (int j = 0; j < r[i].size(); j++) std::cout << r[i][j] << " ";
    std::cout << "\n";
  }
  return 0;
}
```
```bash:実行結果
4 5 6 7
12 17 22 27
20 29 38 47
```
ちゃんと計算できました。
気休め程度にOpenMPで並列化してみました。
しかし1000x1000のベクトル同士の積などとなると非常に時間がかかります。おそらく最適化することで何倍も速くなりますが、本筋と離れるので一旦このまま進みます。

## 3層ニューラルネットワーク
### 各層における信号伝達の実装
入力、重み、バイアスをそれぞれ定義して計算してみます。
C++には行列を扱うデータ型がなく，さすがに扱いにくいのでNdArrayクラスを定義しました。
ここに載せると邪魔なので興味のある方はGithubを見てください。

行列積、四則演算程度の機能しか持たせていません。

基本的に原著と同様の書き方ができるように設計したのであまり見た目が変わらないと思います。
行列積はdotでもよかったのですがより直感的に扱えるように、使わないであろう演算子 '&'をオーバーロードしました。
ソースコード、結果は以下の通りです。

```cpp:main.cpp
namespace myDeepLearning {
using network_t = std::unordered_map<std::string, nc::NdArray<double>>;

network_t init_network() {
  network_t result;
  // Weight
  result["W1"] = {{0.1, 0.3, 0.5}, {0.2, 0.4, 0.6}};
  result["W2"] = {{0.1, 0.4}, {0.2, 0.5}, {0.3, 0.6}};
  result["W3"] = {{0.1, 0.3}, {0.2, 0.4}};

  // Bias
  result["B1"] = {{0.1, 0.2, 0.3}};
  result["B2"] = {{0.1, 0.2}};
  result["B3"] = {{0.1, 0.2}};

  return result;
}

nc::NdArray<double> forward(network_t &network, nc::NdArray<double> &x) {
  auto &&a1 = (x & network["W1"]) + network["B1"];
  auto &&z1 = dl::sigmoid(a1);

  auto &&a2 = (z1 & network["W2"]) + network["B2"];
  auto &&z2 = dl::sigmoid(a2);

  auto &&a3 = (z2 & network["W3"]) + network["B3"];
  return a3;
}

}  // namespace myDeepLearning

namespace mdl = myDeepLearning;

int main() {
  auto &&network = mdl::init_network();
  nc::NdArray x(1, 2);
  x[0] = 1.0;
  x[1] = 0.5;
  auto &&y = mdl::forward(network, x);
  std::cout << y << std::endl;

  return 0;
}
```
```bash
$ ./DLZero
0.316827 0.696279
```

無事、原著と同じ結果を得ることができました。